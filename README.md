LEIA SOBRE - Funcionamento dos Scripts e Bibliotecas

Este projeto automatiza a extra√ß√£o, processamento, an√°lise e exporta√ß√£o de posts do blog da 99App. Abaixo est√° um resumo da l√≥gica de cada script principal e as bibliotecas necess√°rias.

---

üèóÔ∏è Arquitetura da Solu√ß√£o
O projeto √© modular, garantindo que cada etapa do processamento seja executada com efici√™ncia e facilidade de manuten√ß√£o:

1. Orquestra√ß√£o e Extra√ß√£o

 main.py
- Orquestra todo o pipeline: baixa URLs do sitemap, verifica hist√≥rico, extrai conte√∫do dos posts, aplica NLP, exporta para Excel e gerencia logs.
- Inicializa o ChromeDriver para navega√ß√£o automatizada.
- Garante que URLs j√° processadas n√£o sejam repetidas.
- Exporta os dados para m√∫ltiplas abas no Excel: Dados Brutos, Motorista, 99Pay.

crawler.py
- Baixa e filtra URLs do sitemap.
- Extrai t√≠tulo, resumo, data de publica√ß√£o e conte√∫do dos posts usando Selenium e Newspaper3k.
- Categoriza cada URL.

2. Intelig√™ncia e NLP

nlp_utils.py
- Aplica processamento de linguagem natural (NLP) usando spaCy e scikit-learn.
- Identifica topic clusters para cada post com base em palavras-chave.
- Gera a coluna 'topic_clusters' para an√°lise tem√°tica dos posts.

3. Exporta√ß√£o e Gest√£o

exportador.py
- Exporta o DataFrame final para um arquivo Excel (.xlsx) com m√∫ltiplas abas.
- Aba 'Motorista' inclui URLs de /blog/motorista e /blog/99moto.
- Aba '99Pay' inclui URLs de /blog/99pay.
- Aba 'Dados Brutos' cont√©m todos os dados.

gera_historico_urls_do_excel.py (executado uma √∫nica vez)
- Este script √© utilizado para gerar o arquivo de hist√≥rico de URLs processadas a partir de um Excel j√° existente, normalmente apenas uma vez para inicializar o hist√≥rico.
- O arquivo de hist√≥rico (historico_urls_processadas.txt) √© alimentado automaticamente toda vez que o main.py roda, evitando reprocessamento de URLs j√° tratadas.

Reextrai_urls_com_erro.py (executado separadamente)
- Este script √© chamado de forma independente, fora do fluxo principal do main.py.
- Serve para reprocessar URLs que apresentaram erro na extra√ß√£o anterior, utilizando o crawler e exportador para tentar novamente e salvar resultados.

---

üß† Decis√µes T√©cnicas e Curadoria Humana
Diferente de uma abordagem puramente automatizada, este projeto utiliza uma metodologia de IA assistida para garantir resultados acion√°veis:

Taxonomia Direcionada: Para superar as limita√ß√µes de criatividade de modelos gen√©ricos, desenvolvi uma biblioteca pr√≥pria de palavras-chave de contexto. O modelo busca nessa "biblioteca" os termos que melhor se encaixam no conte√∫do lido, garantindo uma categoriza√ß√£o fiel ao universo de neg√≥cios da 99.

Otimiza√ß√£o por Meta-Descri√ß√£o: Por decis√£o de engenharia e performance, o script realiza a categoriza√ß√£o a partir da leitura das meta-descri√ß√µes. Isso garante a captura do resumo estrat√©gico do post e otimiza o tempo de processamento sem perda de qualidade na clusteriza√ß√£o.

Escalabilidade de Dados: O pipeline foi capaz de processar quase 1.000 posts hist√≥ricos. Na execu√ß√£o inicial, o script operou por 11 horas ininterruptas, um investimento de tempo computacional que substitui um esfor√ßo manual que seria humanamente invi√°vel, mantendo a padroniza√ß√£o total da base.


Bibliotecas que precisam ser instaladas (requirements.txt) e seus prop√≥sitos:

1. pandas, numpy: Manipula√ß√£o e an√°lise de dados em DataFrames e arrays.
2. openpyxl, xlsxwriter: Leitura e escrita de arquivos Excel (.xlsx).
3. selenium, webdriver-manager: Automa√ß√£o de navega√ß√£o web para extra√ß√£o de conte√∫do dos posts. O WebDriver pode variar conforme o sistema e recursos dispon√≠veis (ex: Chrome, Firefox, Edge).
4. beautifulsoup4, lxml: Extra√ß√£o e parsing de HTML para obter informa√ß√µes dos posts.
5. requests: Requisi√ß√µes HTTP para baixar o sitemap e p√°ginas web.
6. newspaper3k: Extra√ß√£o alternativa de conte√∫do de not√≠cias/posts.
7. scikit-learn, scipy: Algoritmos de machine learning e c√°lculos de similaridade para an√°lise de t√≥picos.
8. spacy, sentence-transformers: Processamento de linguagem natural (NLP) para identificar clusters tem√°ticos. O modelo NLP pode ser ajustado conforme o sistema e mem√≥ria dispon√≠vel (ex: modelos menores ou maiores, GPU/CPU).
9. python-dateutil, pytz: Manipula√ß√£o de datas e fusos hor√°rios.
10. logging (nativo do Python): Registro de logs do pipeline e dos scripts.
11. Outros utilit√°rios: Auxiliam em tarefas espec√≠ficas conforme listados no requirements.txt.

Observa√ß√£o importante:
- A escolha do WebDriver (selenium) e do modelo NLP (spaCy, transformers) pode variar conforme o sistema operacional, mem√≥ria RAM e recursos dispon√≠veis. Avalie o que melhor atende ao seu ambiente para garantir performance e compatibilidade.

Para instalar todas as depend√™ncias, execute:
    pip install -r requirements.txt

---


Resumo do fluxo:
1. main.py inicia o pipeline, baixa URLs e extrai dados.
2. crawler.py faz a extra√ß√£o e categoriza√ß√£o dos posts.
3. nlp_utils.py aplica an√°lise de t√≥picos (NLP).
4. exportador.py exporta os dados processados para Excel com abas tem√°ticas.
5. Scripts auxiliares (reextrai_urls_com_erro.py e gera_historico_urls_do_excel.py) s√£o executados separadamente conforme necessidade.

Consulte cada script para detalhes espec√≠ficos de par√¢metros e fun√ß√µes.